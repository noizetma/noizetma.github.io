{
  "hash": "30a8496903bf5eee205303c05a5710df",
  "result": {
    "markdown": "---\ntitle: 'Automatic Image Annotation for Mapped Features Detection'\ndate: 2024-10-16\nimage: imgs/iros24.jpg\ndescription: Used in the paper [Automatic Image Annotation for Mapped Features Detection](../publications/iros_2024.qmd) published in 2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)\n\ncitation:\n    type: paper-conference\n    title: 'Automatic Image Annotation for Mapped Features Detection'\n    url: https://ieeexplore.ieee.org/document/10801773\n    container-title: 2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)\n    page-first: 9367\n    page-last: 9373\n    keyword: Location awareness;Image segmentation;Laser radar;Accuracy;Uncertainty;Annotations;Roads;Feature extraction;Vectors;Robustness\n    doi: 10.1109/IROS58592.2024.10801773\n    author:\n    - Maxime Noizet\n    - Philippe Xu\n    - Philippe Bonnifait\n---\n\n[Download](https://datasets.hds.utc.fr/share/vBMPrMDM2wOEd2g){.btn .btn-outline-primary .btn role=\"button\"}\n\n\n# Dataset Overview\n\nIt is designed for training and evaluating pole base detection models for cameras  using automatic annotations instead of manual ones,\n\n\nThe dataset includes a collection of images with automatic annotations generated using three methods proposed in the associated paper for training, as well as a collection of manually annotated images for validation. Additionally, it provides all the necessary data to recompute the automatic annotations using each method.\n\n## Purpose\n\nThis dataset was created to demonstrate the achievable performance of a pole base detector based on YOLOv7, trained using automatic annotations to minimize or eliminate the need for costly human labeling. By showcasing the capabilities and limitations of automatic annotations, as well as the resulting detection performance, This dataset paves the way for generating vast amounts of automatically labeled data and developing detectors specifically optimized for road environments and diverse use cases.\n\nThe final purpose of these detectors is to support autonomous vehicle localization by providing additional positioning information through the association of detections with georeferenced poles from the HD map.\n\n## Dataset Composition\n\nIt consists of two main components:  \n\n1. **Dataset for Object Detection**:  \n   This includes images and their corresponding annotations, enabling direct training and validation of detection models. Additionally, the detection weights used to produce the results presented in the associated paper are provided.  \n\n2. **Raw Data for Automatic Annotation**:  \n   This contains all the necessary data to regenerate the annotations in the training folder, allowing users to reproduce or customize the annotation process.\n\nIt contains the following data:\n\n- **Number of images**: 5,391 image for the training and 939 images for the validation\n- **Image resolution**: 1280x720 pixels\n- **Training annotations**: Each image in the training set is provided with automatic annotations generated through three distinct methods: an image segmentation approach, a LiDAR-based approach, and a map-driven approach.\n- **Validation annotations**: Images in the validation set were annotated by five different humans and validated by an additional one. While efforts were made to ensure accuracy, errors or missed pole bases may still occur. Bollards were intentionally excluded from annotations, as they are not represented in our HD maps. For subsequent localization tasks, we aim to develop a detector specifically designed to detect only mapped elements, which justifies this decision.\n- **Environment**: The images were captured in both urban and rural settings within the small French city of Compiègne (approximately 70,000 residents), showcasing a variety of lighting conditions and weather scenarios.\n\n### Raw data for automatic annotations\n\n- **High-definition vector map**: A CSV file containing the georeferenced positions of all pole bases (except bollards not included in the map) in the city of Compiègne. The map was acquired in 2022, and since roadworks have taken place afterward, some errors may exist.\n- **Pre trained image and pointcloud segmentation weights**: The used weights by lidar-based and image segmentation-based methods are provided.\n- **Lidar pointclouds**: For each image, the corresponding point cloud is provided to extract poles and annotate them using the lidar-based method or to refine the map-based approach.\n- **Sensor calibration**: The training and validation sets include their own calibration parameters, enabling transformations between the map frame, vehicle frame, image frame, and lidar frame.\n- **Vehicle reference poses**: The training set consists of images extracted from three different sequences, while the validated set is derived from two different sequences. The sequence of each image can be identified by its filename, which corresponds to the acquisition timestamp. For each sequence, the reference poses of the vehicle are provided to transform the map data into the vehicle frame for the map-based automatic annotation approaches.\n\n\n# Usage\n\nThis dataset is intended for training, validating, and testing pole base detection algorithms. It can be used with object detection models such as YOLO adding bounding boxes around each pointwise annotations to provide visual context as described in the associated paper.\n\n\nThe dataset is hosted on heudiasyc laboratory dataset platform and can be downloaded directly [here](https://datasets.hds.utc.fr/share/vBMPrMDM2wOEd2g).\n\nThe dataset is provided under the Creative Commons Attribution 4.0 International (CC BY 4.0) License, allowing for both academic and commercial use with proper attribution.\n\nIf you use this dataset in your research or projects, please cite the associated paper as follows:\n\n\n```{latex}\n@misc{noizet2024,\n  author = {Noizet, Maxime and Xu, Philippe and Bonnifait, Philippe},\n  title = {Automatic {Image} {Annotation} for {Mapped} {Features}\n    {Detection}},\n  pages = {9367 - 9373},\n  date = {2024-10-16},\n  url = {https://ieeexplore.ieee.org/document/10801773},\n  doi = {10.1109/IROS58592.2024.10801773},\n  langid = {en},\n  abstract = {Detecting road features is a key enabler for autonomous\n    driving and localization. For instance, a reliable detection of\n    poles which are widespread in road environments can improve\n    localization. Modern deep learning-based perception systems need a\n    significant amount of annotated data. Automatic annotation avoids\n    time-consuming and costly manual annotation. Because automatic\n    methods are prone to errors, managing annotation uncertainty is\n    crucial to ensure a proper learning process. Fusing multiple\n    annotation sources on the same dataset can be an efficient way to\n    reduce the errors. This not only improves the quality of\n    annotations, but also improves the learning of perception models. In\n    this paper, we consider the fusion of three automatic annotation\n    methods in images: feature projection from a high accuracy vector\n    map combined with a lidar, image segmentation and lidar\n    segmentation. Our experimental results demonstrate the significant\n    benefits of multi-modal automatic annotation for pole detection\n    through a comparative evaluation on manually annotated images.\n    Finally, the resulting multi-modal fusion is used to fine-tune an\n    object detection model for pole base detection using unlabeled data,\n    showing overall improvements achieved by enhancing network\n    specialization. The dataset is publicly available.}\n}\n```\n\n\n# Example Visuals\n\nBelow are examples of images from the dataset with annotations.\n\n![Example 1](iros24_images/1657876324690787.jpg){width=30%}\n![Example 2](iros24_images/1657875891665763.jpg){width=30%}\n![Example 3](iros24_images/1657875867664493.jpg){width=30%}\n\n![Example 4](iros24_images/1657875681657883.jpg){width=30%}\n![Example 5](iros24_images/1657875159626889.jpg){width=30%}\n![Example 6](iros24_images/1652168973023933.jpg){width=30%}\n\n![Example 7](iros24_images/1652169111027543.jpg){width=30%}\n![Example 8](iros24_images/1652169199024968.jpg){width=30%}\n![Example 9](iros24_images/1652169469026435.jpg){width=30%}\n\n![Example 10](iros24_images/1652169861030400.jpg){width=30%}\n![Example 11](iros24_images/1652170737036725.jpg){width=30%}\n![Example 12](iros24_images/1652974606185625.jpg){width=30%}\n\n![Example 13](iros24_images/1652974847184955.jpg){width=30%}\n![Example 14](iros24_images/1652975218185893.jpg){width=30%}\n![Example 15](iros24_images/1652976062188942.jpg){width=30%}\n\n![Example 16](iros24_images/1652976161196060.jpg){width=30%}\n![Example 17](iros24_images/1652976245188973.jpg){width=30%}\n![Example 18](iros24_images/1652976324191861.jpg){width=30%}\n\n![Example 19](iros24_images/1656404637974282.jpg){width=30%}\n![Example 20](iros24_images/1656404681975218.jpg){width=30%}\n![Example 21](iros24_images/1656404832985821.jpg){width=30%}\n\n![Example 22](iros24_images/1656404950991653.jpg){width=30%}\n![Example 23](iros24_images/1656405112000765.jpg){width=30%}\n![Example 24](iros24_images/1657111647812218.jpg){width=30%}\n\nThe methods used to generate automatic annotations and the detection results obtained using the dataset are demonstrated in a video available [on YouTube](https://www.youtube.com/watch?v=8cyF0mxi3yU).\n\nThe video showcases a segment of a driving sequence from the dataset used in this study and provides a detailed presentation of the results.\n\nInitially, the video highlights the automatic annotations generated by the three methods proposed in the paper. It then demonstrates the process of merging these annotation sources, using different colors for the crosses representing annotations to indicate the level of consensus among the methods. Specifically, annotations validated by all methods are distinguished from those that are ambiguous.\n\nNext, the video illustrates how black patches were added to address uncertainties in the annotations. Finally, it presents the results of pole base detection using a YOLOv7 neural network. This network was trained on high-consensus automatic annotations, with the input images modified to mask ambiguous objects.\n\n\n# Automatic annotations\n\n## Map-based annotation\n\n## Image segmentation-based annotation\n\n\n## Lidar-based annotation\n\n\nBlaba\n\n# Sensors\n\n- Localization reference system: All vehicles are equipped with a Novatel SPAN-CPT GNSS receiver with RTK capabilities and is loosely coupled with a high-accuracy IMU. GNSS PPK corrections are applied using Novatel’s \\textit{Inertial Explorer} software, enabling centimeter-level accuracy localization for each vehicle. This system provides the reference state used for localization evaluation. For PPK, more than ten reference stations within a 50 km radius around Compiègne are maintained by IGN, the French national institute for geographic information. Additionally, a reference station has been deployed atop one of the university buildings, by using a Septentrio AsteRx SB PRO Connect GNSS receiver. This station can be used for PPK but is generally employed in real-time tests where high-precision localization is required by providing RTK corrections. The pose of each vehicle is provided at a frequency of 50 Hz. Additionally, all dynamic and kinematic information is fully accessible.\n\n- Hesai Pandora sensor kit: This sensor is composed of a lidar delivering 3D scans of the environment using 40 vertically stacked lasers with a non-linear distribution and five monocular cameras located below the lidar. Four of these cameras are grayscale wide-angle with a horizontal field of view of $129^\\circ$, providing full coverage of the vehicle's surroundings as they face the front, left, right, and rear. Consequently, both the lidar and cameras cover the same area. The fifth camera is a front-facing color camera with a horizontal FOV of $52^\\circ$. The cameras are synchronized with the lidar, capturing images when the lasers align with the cameras' focal axis. Both the extrinsic and intrinsic calibrations of all sensors use factory settings. With a theoretical range of 200 meters, the system operates at a frequency of 10 Hz.\n\n",
    "supporting": [
      "datasets_iros2024_files"
    ],
    "filters": [],
    "includes": {}
  }
}