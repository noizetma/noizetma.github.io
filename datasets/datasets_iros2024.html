<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2024-10-16">
<meta name="description" content="Used in the paper Automatic Image Annotation for Mapped Features Detection published in 2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)">

<title>Maxime Noizet - Automatic Image Annotation for Mapped Features Detection</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">Maxime Noizet</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../index.html" rel="" target="">
 <span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../research.html" rel="" target="">
 <span class="menu-text">Research</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../publications/index.html" rel="" target="">
 <span class="menu-text">Publications</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../datasets/index.html" rel="" target="">
 <span class="menu-text">Datasets</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../students.html" rel="" target="">
 <span class="menu-text">Students supervision</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../teaching.html" rel="" target="">
 <span class="menu-text">Teaching</span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools ms-auto">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#dataset-overview" id="toc-dataset-overview" class="nav-link active" data-scroll-target="#dataset-overview">Dataset Overview</a>
  <ul class="collapse">
  <li><a href="#purpose" id="toc-purpose" class="nav-link" data-scroll-target="#purpose">Purpose</a></li>
  <li><a href="#dataset-composition" id="toc-dataset-composition" class="nav-link" data-scroll-target="#dataset-composition">Dataset Composition</a>
  <ul class="collapse">
  <li><a href="#raw-data-for-automatic-annotations" id="toc-raw-data-for-automatic-annotations" class="nav-link" data-scroll-target="#raw-data-for-automatic-annotations">Raw data for automatic annotations</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#usage" id="toc-usage" class="nav-link" data-scroll-target="#usage">Usage</a></li>
  <li><a href="#example-visuals" id="toc-example-visuals" class="nav-link" data-scroll-target="#example-visuals">Example Visuals</a></li>
  <li><a href="#automatic-annotations" id="toc-automatic-annotations" class="nav-link" data-scroll-target="#automatic-annotations">Automatic annotations</a>
  <ul class="collapse">
  <li><a href="#map-based-annotation" id="toc-map-based-annotation" class="nav-link" data-scroll-target="#map-based-annotation">Map-based annotation</a></li>
  <li><a href="#image-segmentation-based-annotation" id="toc-image-segmentation-based-annotation" class="nav-link" data-scroll-target="#image-segmentation-based-annotation">Image segmentation-based annotation</a></li>
  <li><a href="#lidar-based-annotation" id="toc-lidar-based-annotation" class="nav-link" data-scroll-target="#lidar-based-annotation">Lidar-based annotation</a></li>
  </ul></li>
  <li><a href="#sensors" id="toc-sensors" class="nav-link" data-scroll-target="#sensors">Sensors</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Automatic Image Annotation for Mapped Features Detection</h1>
</div>

<div>
  <div class="description">
    Used in the paper <a href="../publications/iros_2024.html">Automatic Image Annotation for Mapped Features Detection</a> published in 2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
  </div>
</div>


<div class="quarto-title-meta">

    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">October 16, 2024</p>
    </div>
  </div>
  
    
  </div>
  

</header>

<p><a href="https://datasets.hds.utc.fr/share/vBMPrMDM2wOEd2g" class="btn btn-outline-primary btn" role="button">Download</a></p>
<section id="dataset-overview" class="level1">
<h1>Dataset Overview</h1>
<p>It is designed for training and evaluating pole base detection models for cameras using automatic annotations instead of manual ones,</p>
<p>The dataset includes a collection of images with automatic annotations generated using three methods proposed in the associated paper for training, as well as a collection of manually annotated images for validation. Additionally, it provides all the necessary data to recompute the automatic annotations using each method.</p>
<section id="purpose" class="level2">
<h2 class="anchored" data-anchor-id="purpose">Purpose</h2>
<p>This dataset was created to demonstrate the achievable performance of a pole base detector based on YOLOv7, trained using automatic annotations to minimize or eliminate the need for costly human labeling. By showcasing the capabilities and limitations of automatic annotations, as well as the resulting detection performance, This dataset paves the way for generating vast amounts of automatically labeled data and developing detectors specifically optimized for road environments and diverse use cases.</p>
<p>The final purpose of these detectors is to support autonomous vehicle localization by providing additional positioning information through the association of detections with georeferenced poles from the HD map.</p>
</section>
<section id="dataset-composition" class="level2">
<h2 class="anchored" data-anchor-id="dataset-composition">Dataset Composition</h2>
<p>It consists of two main components:</p>
<ol type="1">
<li><p><strong>Dataset for Object Detection</strong>:<br>
This includes images and their corresponding annotations, enabling direct training and validation of detection models. Additionally, the detection weights used to produce the results presented in the associated paper are provided.</p></li>
<li><p><strong>Raw Data for Automatic Annotation</strong>:<br>
This contains all the necessary data to regenerate the annotations in the training folder, allowing users to reproduce or customize the annotation process.</p></li>
</ol>
<p>It contains the following data:</p>
<ul>
<li><strong>Number of images</strong>: 5,391 image for the training and 939 images for the validation</li>
<li><strong>Image resolution</strong>: 1280x720 pixels</li>
<li><strong>Training annotations</strong>: Each image in the training set is provided with automatic annotations generated through three distinct methods: an image segmentation approach, a LiDAR-based approach, and a map-driven approach.</li>
<li><strong>Validation annotations</strong>: Images in the validation set were annotated by five different humans and validated by an additional one. While efforts were made to ensure accuracy, errors or missed pole bases may still occur. Bollards were intentionally excluded from annotations, as they are not represented in our HD maps. For subsequent localization tasks, we aim to develop a detector specifically designed to detect only mapped elements, which justifies this decision.</li>
<li><strong>Environment</strong>: The images were captured in both urban and rural settings within the small French city of Compiègne (approximately 70,000 residents), showcasing a variety of lighting conditions and weather scenarios.</li>
</ul>
<section id="raw-data-for-automatic-annotations" class="level3">
<h3 class="anchored" data-anchor-id="raw-data-for-automatic-annotations">Raw data for automatic annotations</h3>
<ul>
<li><strong>High-definition vector map</strong>: A CSV file containing the georeferenced positions of all pole bases (except bollards not included in the map) in the city of Compiègne. The map was acquired in 2022, and since roadworks have taken place afterward, some errors may exist.</li>
<li><strong>Pre trained image and pointcloud segmentation weights</strong>: The used weights by lidar-based and image segmentation-based methods are provided.</li>
<li><strong>Lidar pointclouds</strong>: For each image, the corresponding point cloud is provided to extract poles and annotate them using the lidar-based method or to refine the map-based approach.</li>
<li><strong>Sensor calibration</strong>: The training and validation sets include their own calibration parameters, enabling transformations between the map frame, vehicle frame, image frame, and lidar frame.</li>
<li><strong>Vehicle reference poses</strong>: The training set consists of images extracted from three different sequences, while the validated set is derived from two different sequences. The sequence of each image can be identified by its filename, which corresponds to the acquisition timestamp. For each sequence, the reference poses of the vehicle are provided to transform the map data into the vehicle frame for the map-based automatic annotation approaches.</li>
</ul>
</section>
</section>
</section>
<section id="usage" class="level1">
<h1>Usage</h1>
<p>This dataset is intended for training, validating, and testing pole base detection algorithms. It can be used with object detection models such as YOLO adding bounding boxes around each pointwise annotations to provide visual context as described in the associated paper.</p>
<p>The dataset is hosted on heudiasyc laboratory dataset platform and can be downloaded directly <a href="https://datasets.hds.utc.fr/share/vBMPrMDM2wOEd2g">here</a>.</p>
<p>The dataset is provided under the Creative Commons Attribution 4.0 International (CC BY 4.0) License, allowing for both academic and commercial use with proper attribution.</p>
<p>If you use this dataset in your research or projects, please cite the associated paper as follows:</p>
</section>
<section id="example-visuals" class="level1">
<h1>Example Visuals</h1>
<p>Below are examples of images from the dataset with annotations.</p>
<p><img src="iros24_images/1657876324690787.jpg" class="img-fluid" style="width:30.0%" alt="Example 1"> <img src="iros24_images/1657875891665763.jpg" class="img-fluid" style="width:30.0%" alt="Example 2"> <img src="iros24_images/1657875867664493.jpg" class="img-fluid" style="width:30.0%" alt="Example 3"></p>
<p><img src="iros24_images/1657875681657883.jpg" class="img-fluid" style="width:30.0%" alt="Example 4"> <img src="iros24_images/1657875159626889.jpg" class="img-fluid" style="width:30.0%" alt="Example 5"> <img src="iros24_images/1652168973023933.jpg" class="img-fluid" style="width:30.0%" alt="Example 6"></p>
<p><img src="iros24_images/1652169111027543.jpg" class="img-fluid" style="width:30.0%" alt="Example 7"> <img src="iros24_images/1652169199024968.jpg" class="img-fluid" style="width:30.0%" alt="Example 8"> <img src="iros24_images/1652169469026435.jpg" class="img-fluid" style="width:30.0%" alt="Example 9"></p>
<p><img src="iros24_images/1652169861030400.jpg" class="img-fluid" style="width:30.0%" alt="Example 10"> <img src="iros24_images/1652170737036725.jpg" class="img-fluid" style="width:30.0%" alt="Example 11"> <img src="iros24_images/1652974606185625.jpg" class="img-fluid" style="width:30.0%" alt="Example 12"></p>
<p><img src="iros24_images/1652974847184955.jpg" class="img-fluid" style="width:30.0%" alt="Example 13"> <img src="iros24_images/1652975218185893.jpg" class="img-fluid" style="width:30.0%" alt="Example 14"> <img src="iros24_images/1652976062188942.jpg" class="img-fluid" style="width:30.0%" alt="Example 15"></p>
<p><img src="iros24_images/1652976161196060.jpg" class="img-fluid" style="width:30.0%" alt="Example 16"> <img src="iros24_images/1652976245188973.jpg" class="img-fluid" style="width:30.0%" alt="Example 17"> <img src="iros24_images/1652976324191861.jpg" class="img-fluid" style="width:30.0%" alt="Example 18"></p>
<p><img src="iros24_images/1656404637974282.jpg" class="img-fluid" style="width:30.0%" alt="Example 19"> <img src="iros24_images/1656404681975218.jpg" class="img-fluid" style="width:30.0%" alt="Example 20"> <img src="iros24_images/1656404832985821.jpg" class="img-fluid" style="width:30.0%" alt="Example 21"></p>
<p><img src="iros24_images/1656404950991653.jpg" class="img-fluid" style="width:30.0%" alt="Example 22"> <img src="iros24_images/1656405112000765.jpg" class="img-fluid" style="width:30.0%" alt="Example 23"> <img src="iros24_images/1657111647812218.jpg" class="img-fluid" style="width:30.0%" alt="Example 24"></p>
<p>The methods used to generate automatic annotations and the detection results obtained using the dataset are demonstrated in a video available <a href="https://www.youtube.com/watch?v=8cyF0mxi3yU">on YouTube</a>.</p>
<p>The video showcases a segment of a driving sequence from the dataset used in this study and provides a detailed presentation of the results.</p>
<p>Initially, the video highlights the automatic annotations generated by the three methods proposed in the paper. It then demonstrates the process of merging these annotation sources, using different colors for the crosses representing annotations to indicate the level of consensus among the methods. Specifically, annotations validated by all methods are distinguished from those that are ambiguous.</p>
<p>Next, the video illustrates how black patches were added to address uncertainties in the annotations. Finally, it presents the results of pole base detection using a YOLOv7 neural network. This network was trained on high-consensus automatic annotations, with the input images modified to mask ambiguous objects.</p>
</section>
<section id="automatic-annotations" class="level1">
<h1>Automatic annotations</h1>
<section id="map-based-annotation" class="level2">
<h2 class="anchored" data-anchor-id="map-based-annotation">Map-based annotation</h2>
<p>A first approach is to use a 2D High Definition (HD) vector map along with a high accuracy localization system. The poles georeferenced within the field of view of the camera are extracted from the map and projected onto the images. Even though the poles contained in the map concern furniture (such as traffic signs, traffic lights or streetlamps) which is stable over time, the map can still become outdated. So, false positives and negatives are inevitable. To project the 2D map features at the ground level, a lidar is used to estimate the ground and check for occluded pole bases. To mitigate the risk of false positives and enhance positioning accuracy, we refrain from annotating distant poles, even if it may introduce the possibility of false negatives.</p>
<p>More details are provided in <a href="../publications/iv_2023.html">the conference paper</a> published at IV2023 and in the thesis manuscript.</p>
</section>
<section id="image-segmentation-based-annotation" class="level2">
<h2 class="anchored" data-anchor-id="image-segmentation-based-annotation">Image segmentation-based annotation</h2>
<p>We use the HRNet image semantic segmentation neural network and pre-trained on the BDD100K dataset to extract pole bases from estimated segmentation masks. We combine all pole-related classes to form entire pole clusters to check if they are connected to ground pixels. It ensures that only large clusters of pole pixels are considered, thus minimizing the influence of poor segmentation. However, some poles can be merged during clustering. To avoid this, we have chosen to extract any small clusters of pixels lying on the ground. Some wrong annotations can be introduced in the process: some correspond to bollards and some to other errors. However, it is capable of annotating more pole bases than the map-based approach.</p>
<p>More details are provided in <a href="../publications/itsc_2023.html">the conference paper</a> published at ITSC2023 and in the thesis manuscript.</p>
</section>
<section id="lidar-based-annotation" class="level2">
<h2 class="anchored" data-anchor-id="lidar-based-annotation">Lidar-based annotation</h2>
<p>We segment the point cloud using Cylinder3D, a 3D convolution network pre-trained on SemanticKITTI. Then, we group the points classified as poles into clusters to identify each pole individually and fit a 3D bounding box. The pole base 3D coordinates corresponds to the center of the bottom face of the bounding box, which is in turn projected on the image to generate the annotation. Similarly to the image segmentation annotation, the neural network trained for the lidar segmentation consider that bollards belong semantically to the pole class. Far poles are not annotated due to the data sparsity and the lack of points.</p>
<p>More details are provided in <a href="../publications/iros_2024.html">the conference paper</a> published at IROS2024 and in the thesis manuscript.</p>
</section>
</section>
<section id="sensors" class="level1">
<h1>Sensors</h1>
<ul>
<li><p>Localization reference system: All vehicles are equipped with a Novatel SPAN-CPT GNSS receiver with RTK capabilities and is loosely coupled with a high-accuracy IMU. GNSS PPK corrections are applied using Novatel’s software, enabling centimeter-level accuracy localization for each vehicle. This system provides the reference state used for localization evaluation. For PPK, more than ten reference stations within a 50 km radius around Compiègne are maintained by IGN, the French national institute for geographic information. Additionally, a reference station has been deployed atop one of the university buildings, by using a Septentrio AsteRx SB PRO Connect GNSS receiver. This station can be used for PPK but is generally employed in real-time tests where high-precision localization is required by providing RTK corrections. The pose of each vehicle is provided at a frequency of 50 Hz. Additionally, all dynamic and kinematic information is fully accessible.</p></li>
<li><p>Hesai Pandora sensor kit: This sensor is composed of a lidar delivering 3D scans of the environment using 40 vertically stacked lasers with a non-linear distribution and five monocular cameras located below the lidar. Four of these cameras are grayscale wide-angle with a horizontal field of view of <span class="math inline">\(129^\circ\)</span>, providing full coverage of the vehicle’s surroundings as they face the front, left, right, and rear. Consequently, both the lidar and cameras cover the same area. The fifth camera is a front-facing color camera with a horizontal FOV of <span class="math inline">\(52^\circ\)</span>. The cameras are synchronized with the lidar, capturing images when the lasers align with the cameras’ focal axis. Both the extrinsic and intrinsic calibrations of all sensors use factory settings. With a theoretical range of 200 meters, the system operates at a frequency of 10 Hz.</p></li>
</ul>


</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@inproceedings{noizet2024,
  author = {Noizet, Maxime and Xu, Philippe and Bonnifait, Philippe},
  title = {Automatic {Image} {Annotation} for {Mapped} {Features}
    {Detection}},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent
    Robots and Systems (IROS)},
  pages = {9367 - 9373},
  date = {2024-10-16},
  url = {https://ieeexplore.ieee.org/document/10801773},
  doi = {10.1109/IROS58592.2024.10801773},
  langid = {en}
}
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-noizet2024" class="csl-entry quarto-appendix-citeas" role="listitem">
Noizet, Maxime, Philippe Xu, and Philippe Bonnifait. 2024.
<span>“Automatic Image Annotation for Mapped Features Detection.”</span>
In <em>2024 IEEE/RSJ International Conference on Intelligent Robots and
Systems (IROS)</em>, 9367–73. <a href="https://doi.org/10.1109/IROS58592.2024.10801773">https://doi.org/10.1109/IROS58592.2024.10801773</a>.
</div></div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>