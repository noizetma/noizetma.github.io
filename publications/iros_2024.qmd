---
title: 'Automatic Image Annotation for Mapped Features Detection'
date: 2024-10-16
affiliations:
    - id: 1
      name: Heudiasyc UMR CNRS 7253, Université de technologie de Compiègne
      department: Haudiasyc UMR CNRSS 7253
      city: Compiègne
      country: FR
    - id: 2
      name: U2IS, ENSTA Paris, Institut Polytechnique de Paris
      department: U2IS
      city: Palaiseau
      country: FR
author:
    - name: Maxime Noizet
      orcid: 0009-0008-9321-9410
      affiliation:
        - ref: 1
    - name: Philippe Xu
      orcid: 0000-0001-7397-4808
      affiliation:
        - ref: 1
        - ref: 2
    - name: Philippe Bonnifait
      orcid: 0000-0002-5842-1399
      affiliation:
        - ref: 1
image: imgs/iros24.jpg
description: Published in 2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
abstract: >
  Detecting road features is a key enabler for autonomous driving and localization. For instance, a reliable detection of poles which are widespread in road environments can improve localization. Modern deep learning-based perception systems need a significant amount of annotated data. Automatic annotation avoids time-consuming and costly manual annotation. Because automatic methods are prone to errors, managing annotation uncertainty is crucial to ensure a proper learning process. Fusing multiple annotation sources on the same dataset can be an efficient way to reduce the errors. This not only improves the quality of annotations, but also improves the learning of perception models. In this paper, we consider the fusion of three automatic annotation methods in images: feature projection from a high accuracy vector map combined with a lidar, image segmentation and lidar segmentation. Our experimental results demonstrate the significant benefits of multi-modal automatic annotation for pole detection through a comparative evaluation on manually annotated images. Finally, the resulting multi-modal fusion is used to fine-tune an object detection model for pole base detection using unlabeled data, showing overall improvements achieved by enhancing network specialization. The dataset is publicly available. 

citation:
    type: paper-conference
    title: 'Automatic Image Annotation for Mapped Features Detection'
    url: https://ieeexplore.ieee.org/document/10801773
    container-title: 2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
    page-first: 9367
    page-last: 9373
    keyword: Location awareness;Image segmentation;Laser radar;Accuracy;Uncertainty;Annotations;Roads;Feature extraction;Vectors;Robustness
    doi: 10.1109/IROS58592.2024.10801773

categories:
  - Conference Paper

---



This paper is available [on arXiv](https://arxiv.org/abs/2412.10438).

The results are demonstrated in a video available [on YouTube](https://www.youtube.com/watch?v=8cyF0mxi3yU).

The video showcases a segment of a driving sequence from the dataset used in this study and provides a detailed presentation of the results.

Initially, the video highlights the automatic annotations generated by the three methods proposed in the paper. It then demonstrates the process of merging these annotation sources, using different colors for the crosses representing annotations to indicate the level of consensus among the methods. Specifically, annotations validated by all methods are distinguished from those that are ambiguous.

Next, the video illustrates how black patches were added to address uncertainties in the annotations. Finally, it presents the results of pole base detection using a YOLOv7 neural network. This network was trained on high-consensus automatic annotations, with the input images modified to mask ambiguous objects.